{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we have to import all needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import glob2\n",
    "import librosa\n",
    "import numpy as np\n",
    "from librosa.display import specshow\n",
    "from librosa.feature import mfcc\n",
    "from librosa.feature import melspectrogram\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extract_features()` will be used to load file and draw and return coresponding spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file, mfcc_n=16):\n",
    "    soundfile, samplerate = librosa.core.load(file)\n",
    "    spectogram = melspectrogram(y=soundfile, n_fft=2048, n_mels=128, hop_length=1040)\n",
    "    spectogram = librosa.power_to_db(spectogram, ref=np.max)\n",
    "    return spectogram\n",
    "\n",
    "all_files = glob2.glob('./**/*.mp3')\n",
    "extract_features(all_files[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    new_name = file.replace('.mp3', '.npy').replace('./','records/')\n",
    "    directory = os.path.basename(os.path.dirname(new_name))\n",
    "    if not os.path.isdir(os.path.join('records', directory)):\n",
    "        os.mkdir(os.path.join('records', directory))\n",
    "    features = extract_features(file)\n",
    "    np.save(new_name, features, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use joblib to parallelize preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Parallel(n_jobs=5)(delayed(preprocess)(x) for x in tqdm(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to check if data was properly saved and make some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = defaultdict(int)\n",
    "min_shape = np.inf\n",
    "for file in all_files:\n",
    "    npy_file = file.replace('.mp3','.npy').replace('./', 'records/')\n",
    "    data = np.load(npy_file) # check if correct saved\n",
    "    min_shape = min(min_shape, data.shape[1])\n",
    "    last_shape = data.shape\n",
    "    classes[os.path.basename(os.path.dirname(file))] += 1\n",
    "plt.bar(classes.keys(), classes.values())\n",
    "plt.xticks(rotation=90)\n",
    "print(min_shape)\n",
    "plt.show()\n",
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are redy to split files to train and test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_files = glob2.glob('records/**/*.npy')\n",
    "file_train, file_test = train_test_split(pre_files, test_size=0.33, shuffle=True)\n",
    "for file in file_test:\n",
    "    new_name = file.replace('records', 'test')\n",
    "    directory = os.path.basename(os.path.dirname(new_name))\n",
    "    if not os.path.isdir(os.path.join('test', directory)):\n",
    "        os.mkdir(os.path.join('test', directory))\n",
    "    copyfile(file, new_name)\n",
    "\n",
    "for file in file_train:\n",
    "    new_name = file.replace('records', 'train')\n",
    "    directory = os.path.basename(os.path.dirname(new_name))\n",
    "    if not os.path.isdir(os.path.join('train', directory)):\n",
    "        os.mkdir(os.path.join('train', directory))\n",
    "    copyfile(file, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob2.glob('train/**/*.npy')), len(glob2.glob('test/**/*.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates CNN model with 3 convolution layers and 2 dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(256, kernel_size=4, strides=2, activation='relu',\n",
    "                     data_format='channels_last',\n",
    "                     input_shape=(636, 128)))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Conv1D(256, kernel_size=4, strides=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Conv1D(512, kernel_size=4, strides=2, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(13, activation='softmax'))\n",
    "    \n",
    "              \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelBinarizer()\n",
    "le.fit(list(classes.keys()))\n",
    "le.classes_\n",
    "le.transform(['Samba', 'Rumba', 'Jive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class(file_name):\n",
    "    return os.path.basename(os.path.dirname(file_name))\n",
    "\n",
    "extract_class('test/Samba/118806.npy'), extract_class('test/Rumba/118806.npy'), extract_class('test/Jive/118806.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load all test data to one numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = glob2.glob('train/**/*.npy')\n",
    "X_train = np.empty((len(train_data), 636, 128))\n",
    "y_train = np.empty((len(train_data), 13))\n",
    "for idx, file in tqdm(enumerate(train_data)):\n",
    "    data = np.transpose(np.load(pre_files[0])[:,:636])\n",
    "    data = (data - np.mean(data)) / np.std(data)\n",
    "    X_train[idx, :, :] = data\n",
    "    y = np.zeros(13)\n",
    "    example_class = extract_class(file)\n",
    "    y_train[idx,:] = le.transform([example_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(X_train), np.min(X_train))\n",
    "print(np.max(X_train[1]), np.min(X_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And start learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=55, batch_size=32, callbacks=[TensorBoard(log_dir='./logs', \n",
    "                                                                             histogram_freq=0,\n",
    "                                                                             batch_size=32, \n",
    "                                                                             write_graph=True,\n",
    "                                                                             write_grads=False, \n",
    "                                                                             write_images=False,\n",
    "                                                                             embeddings_freq=0, \n",
    "                                                                             embeddings_layer_names=None,\n",
    "                                                                             embeddings_metadata=None, \n",
    "                                                                             embeddings_data=None, \n",
    "                                                                             update_freq='epoch')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dance_style_recognition.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We similary load teat data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = glob2.glob('test/**/*.npy')\n",
    "X_test = np.empty((len(test_data), 636, 128))\n",
    "y_true = np.empty((len(test_data), 13))\n",
    "for idx, file in tqdm(enumerate(test_data)):\n",
    "    data = np.transpose(np.load(file)[:,:636])\n",
    "    data = (data - np.mean(data)) / np.std(data)\n",
    "    X_test[idx, :, :] = data\n",
    "    y = np.zeros(13)\n",
    "    example_class = extract_class(file)\n",
    "    y_true[idx,:] = le.transform([example_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true = le.inverse_transform(y_true)\n",
    "label_pred = le.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(label_true[:10])\n",
    "print(label_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we calculate accuracy and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(label_true, label_pred)\n",
    "print('Acc: ', acc)\n",
    "confmat = confusion_matrix(label_true, label_pred)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_confusion_matrix(confmat, le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check styles distribution in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = glob2.glob('test/**/*.npy')\n",
    "print(len(test_data))\n",
    "test_c = defaultdict(int)\n",
    "for file in test_data:\n",
    "    test_c[extract_class(file)] += 1\n",
    "plt.bar(test_c.keys(), test_c.values())\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = glob2.glob('train/**/*.npy')\n",
    "print(len(train_data))\n",
    "train_c = defaultdict(int)\n",
    "for file in train_data:\n",
    "    train_c[extract_class(file)] += 1\n",
    "plt.bar(train_c.keys(), train_c.values())\n",
    "plt.xticks(rotation=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
